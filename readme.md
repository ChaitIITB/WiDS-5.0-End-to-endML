# ðŸŒ¿ WiDS Week 3: Deep Learning â€” Let the Model Learn the Features

![Difficulty](https://img.shields.io/badge/Difficulty-Advanced-purple)
![Time](https://img.shields.io/badge/Time-4--5%20Hours-blue)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

## ðŸŽ¯ The Mission

Last week, you built â€œold schoolâ€ ML models.

They worked â€” but they struggled.

This week, you will build something different:

> **A model that learns its own features directly from images.**

You will train your first **Convolutional Neural Network (CNN)** and then upgrade it using **Transfer Learning** to reach **85â€“90% accuracy**.

This is the point where Machine Learning stops being theory â€” and starts feeling powerful.

---

## ðŸ§  Why Do We Need Deep Learning?

Classical models require hand-crafted features.

CNNs do this automatically.

They learn patterns like:

- âœ” edges  
- âœ” textures  
- âœ” spots / lesions  
- âœ” disease shapes on leaves  

And they stack these patterns into understanding:

> â€œThis leaf looks like late blight.â€

CNNs donâ€™t flatten the image â€” they **preserve spatial structure**.

---

## ðŸš€ Our Two-Stage Plan

### â­ Stage 1 â€” Build a Simple CNN (From Scratch)

Youâ€™ll train a small CNN to understand:

- Convolution  
- Pooling  
- Activation functions  
- Overfitting  

Expected accuracy: **70â€“80%**

---

### ðŸ”¥ Stage 2 â€” Transfer Learning (The Real Power Move)

Instead of starting from zero, we use a model trained on **millions of images**, such as:

- MobileNet  
- ResNet  
- EfficientNet  

We freeze most layers, replace the final classifier, and fine-tune.

Expected accuracy: **85â€“90%+**

---

## ðŸ§© The Algorithm (Pseudocode)

```text
ALGORITHM Deep_Disease_Detection:

    1. LOAD dataset (train + validation)

    2. PREPROCESS:
       - Resize images to 224x224
       - Normalize pixel values to [0,1]
       - Apply Data Augmentation (flip/rotate/zoom)

    3. MODEL A: Simple CNN
       - Conv -> ReLU -> MaxPool
       - Conv -> ReLU -> MaxPool
       - Flatten
       - Dense -> Softmax

       Train for ~10 epochs
       Record accuracy

    4. MODEL B: Transfer Learning
       - Load pretrained backbone (e.g., MobileNetV2)
       - Freeze backbone layers
       - Add new classification head:
           GlobalAveragePooling -> Dense -> Softmax

       Train only head (~5 epochs)
       Then optionally unfreeze last few layers and fine-tune.

    5. EVALUATE:
       - Accuracy
       - Confusion Matrix
       - Compare with Week 2 baseline

    6. REFLECT:
       - Why did transfer learning perform better?
       - Where does model still fail?
```

---

## ðŸ§ª Your Tasks

### âœ… Task 1 â€” Train a Simple CNN
Goal: feel the limitations and understand overfitting.

### âœ… Task 2 â€” Apply Transfer Learning
Goal: boost accuracy with fewer parameters.

### âœ… Task 3 â€” Compare
Write down:

- Shallow Model Accuracy (Week 2)  
- CNN Accuracy  
- Transfer Learning Accuracy  

If your deep model doesnâ€™t clearly beat Week 2:

> something is wrong â€” revisit preprocessing or labels.

---

## ðŸ“Š Evaluation Checklist

Before submitting, make sure you generated:

- âœ” accuracy score  
- âœ” confusion matrix  
- âœ” training vs validation curves  
- âœ” short reflection (2â€“3 lines)

---

## ðŸ“š Resources (Curated + Short)

### ðŸŽ¥ Concept explainers
- Neural Networks visually explained (3Blue1Brown):  
  https://www.youtube.com/watch?v=aircAruvnKk
- Convolutions explained simply:  
  https://poloclub.github.io/cnn-explainer/

### ðŸ§­ Transfer Learning Guides
- Transfer Learning with TensorFlow (Keras):  
  https://www.tensorflow.org/tutorials/images/transfer_learning

### ðŸ›  Data Augmentation
- Torchvision transforms:  
  https://pytorch.org/vision/stable/transforms.html

---

## ðŸ What Youâ€™ve Learned

By the end of Week 3, youâ€™ll understand:

- why flattening destroys spatial information  
- what convolutions actually do  
- why pretrained networks are game-changers  
- how deep models move beyond â€œguessing patternsâ€  

And most importantly:

> **You now have a real, working deep learning model.**

Week 4 will take this one step closer to production ðŸ˜‰
